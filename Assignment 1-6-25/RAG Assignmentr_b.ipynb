{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b23a5c0",
   "metadata": {},
   "source": [
    "# RAG Assignment Part b\n",
    "In this assignment, I implemented a complete Retrieval-Augmented Generation (RAG) pipeline using LangChain and Milvus for semantic search, and OpenAI’s LLM for question answering. The process begins by loading a PDF file using PyPDFLoader, and then splitting it into manageable text chunks using the RecursiveCharacterTextSplitter. This allows the model to handle large documents efficiently by breaking them into overlapping sections of fixed size, ensuring context is preserved across chunks.\n",
    "\n",
    "Next, I created a vector store using the Milvus vector database. For this, I used OpenAIEmbeddings to convert the text chunks into high-dimensional embedding vectors. The vector store is initialized with different index types (FLAT, IVF_FLAT, and HNSW) to compare performance across search algorithms. I added the embedded documents into Milvus and performed a similarity search to retrieve the top k most relevant chunks for a user query. The retrieval process also outputs the retrieval time and average similarity score as performance metrics.\n",
    "\n",
    "To enhance the quality of the retrieved documents, I applied Maximum Marginal Relevance (MMR) reranking, which balances relevance and diversity using a parameter lambda_mult. This is done by converting the Milvus store into a retriever using MMR search, fetching a larger number of documents (fetch_k) and selecting the most diverse yet relevant subset (k).\n",
    "\n",
    "Using the reranked documents, I then passed the query to a RetrievalQA chain with an OpenAI LLM (temperature set to 0 for deterministic responses). The model generated a natural language answer based on the retrieved context. Finally, the generated answer was saved in a .docx file using the python-docx library, with separate output files for each index type to support comparison.\n",
    "\n",
    "Overall, this assignment helped me understand and implement each stage of a RAG system: document preprocessing, vectorization, semantic search, reranking, LLM answering, and result saving—demonstrating an end-to-end approach to building intelligent question-answering systems from unstructured documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0e3766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Testing Index Type: FLAT =====\n",
      "Retrieval time: 0.5627 seconds\n",
      "Doc 1: score = 1.0782\n",
      "Doc 2: score = 1.1332\n",
      "Doc 3: score = 1.1369\n",
      "Doc 4: score = 1.1412\n",
      "Doc 5: score = 1.1497\n",
      "Average similarity score: 1.1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/ctmf1pgj1_j32tl4cy4gv86w0000gn/T/ipykernel_3135/918327985.py:57: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  reranked_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MMR Reranked Docs:\n",
      "Doc 1 Preview: 2.1 What Is Statistical Learning? 21\n",
      "Y ears of Education Seniority\n",
      "Income\n",
      "FIGURE 2.4. A linear model...\n",
      "Doc 2 Preview: rest.\n",
      "Comparison to Logistic Regression\n",
      "As a comparison, we can also fit a logistic regression model...\n",
      "Doc 3 Preview: learning method increases, we observe a monotone decrease in the training\n",
      "MSE and aU-shape in the te...\n",
      "Doc 4 Preview: shortly bygeneralized additive models. Neural networksgained popularity\n",
      "in the 1980s, andsupport vec...\n",
      "Doc 5 Preview: known as ageneralized linear model(GLM). Thus, linear regression, logisticgeneralized\n",
      "linear modelre...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/ctmf1pgj1_j32tl4cy4gv86w0000gn/T/ipykernel_3135/918327985.py:66: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), retriever=retriever)\n",
      "/var/folders/20/ctmf1pgj1_j32tl4cy4gv86w0000gn/T/ipykernel_3135/918327985.py:67: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Answer:  Regularization in machine learning is a technique used to prevent overfitting in a model. It involves adding a penalty term to the cost function, which penalizes large values of the model parameters. This helps to reduce the complexity of the model and prevent it from fitting too closely to the training data, which can lead to poor performance on new data. Regularization is commonly used in linear regression, logistic regression, and other models to improve their generalization ability.\n",
      "\n",
      "===== Testing Index Type: IVF_FLAT =====\n",
      "Retrieval time: 0.5309 seconds\n",
      "Doc 1: score = 1.0785\n",
      "Doc 2: score = 1.1332\n",
      "Doc 3: score = 1.1368\n",
      "Doc 4: score = 1.1415\n",
      "Doc 5: score = 1.1497\n",
      "Average similarity score: 1.1279\n",
      "\n",
      "MMR Reranked Docs:\n",
      "Doc 1 Preview: 2.1 What Is Statistical Learning? 21\n",
      "Y ears of Education Seniority\n",
      "Income\n",
      "FIGURE 2.4. A linear model...\n",
      "Doc 2 Preview: rest.\n",
      "Comparison to Logistic Regression\n",
      "As a comparison, we can also fit a logistic regression model...\n",
      "Doc 3 Preview: learning method increases, we observe a monotone decrease in the training\n",
      "MSE and aU-shape in the te...\n",
      "Doc 4 Preview: shortly bygeneralized additive models. Neural networksgained popularity\n",
      "in the 1980s, andsupport vec...\n",
      "Doc 5 Preview: known as ageneralized linear model(GLM). Thus, linear regression, logisticgeneralized\n",
      "linear modelre...\n",
      "\n",
      "LLM Answer:  Regularization in machine learning is a technique used to prevent overfitting in a model. It involves adding a penalty term to the cost function, which penalizes large values of the model parameters. This helps to reduce the complexity of the model and prevent it from fitting too closely to the training data, which can lead to poor performance on new data. Regularization is commonly used in linear regression, logistic regression, and other models to improve their generalization ability.\n",
      "\n",
      "===== Testing Index Type: HNSW =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [create_index], <MilvusException: (code=65535, message=invalid index type: HNSW, local mode only support FLAT IVF_FLAT AUTOINDEX: )>, <Time:{'RPC start': '2025-06-10 00:13:50.024840', 'RPC error': '2025-06-10 00:13:50.025244'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time: 0.3492 seconds\n",
      "Doc 1: score = 1.0777\n",
      "Doc 2: score = 1.1353\n",
      "Doc 3: score = 1.1368\n",
      "Doc 4: score = 1.1412\n",
      "Doc 5: score = 1.1488\n",
      "Average similarity score: 1.1280\n",
      "\n",
      "MMR Reranked Docs:\n",
      "Doc 1 Preview: 2.1 What Is Statistical Learning? 21\n",
      "Y ears of Education Seniority\n",
      "Income\n",
      "FIGURE 2.4. A linear model...\n",
      "Doc 2 Preview: rest.\n",
      "Comparison to Logistic Regression\n",
      "As a comparison, we can also fit a logistic regression model...\n",
      "Doc 3 Preview: learning method increases, we observe a monotone decrease in the training\n",
      "MSE and aU-shape in the te...\n",
      "Doc 4 Preview: shortly bygeneralized additive models. Neural networksgained popularity\n",
      "in the 1980s, andsupport vec...\n",
      "Doc 5 Preview: known as ageneralized linear model(GLM). Thus, linear regression, logisticgeneralized\n",
      "linear modelre...\n",
      "\n",
      "LLM Answer:  Regularization in machine learning is a technique used to prevent overfitting in a model. It involves adding a penalty term to the cost function, which penalizes large values of the model parameters. This helps to reduce the complexity of the model and prevent it from fitting too closely to the training data, which can lead to poor performance on new data. Regularization is commonly used in linear regression, logistic regression, and other models to improve their generalization ability.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "# 1. Load and split PDF\n",
    "def load_and_split_pdf(file_path, max_pages=200, chunk_size=1000, chunk_overlap=50):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()[:max_pages]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = splitter.split_documents(pages)\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "# 2. Create vector store and add documents\n",
    "def create_vector_store(index_type, split_docs, embeddings):\n",
    "    store = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"uri\": \"./milvus_example01.db\"},\n",
    "        index_params={\"index_type\": index_type, \"metric_type\": \"L2\"},\n",
    "        auto_id=True,\n",
    "        drop_old=True\n",
    "    )\n",
    "    store.add_documents(split_docs)\n",
    "    return store\n",
    "\n",
    "\n",
    "# 3. Perform similarity search and return results + metrics\n",
    "def perform_similarity_search(store, query, k=5):\n",
    "    start = time.time()\n",
    "    results = store.similarity_search_with_score(query, k=k)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Retrieval time: {end - start:.4f} seconds\")\n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"Doc {i+1}: score = {score:.4f}\")\n",
    "\n",
    "    avg_score = sum(score for _, score in results) / len(results)\n",
    "    print(f\"Average similarity score: {avg_score:.4f}\")\n",
    "\n",
    "    return results, end - start, avg_score\n",
    "\n",
    "\n",
    "# 4. Apply MMR reranking\n",
    "def rerank_with_mmr(store, query, k=5, fetch_k=10, lambda_mult=0.5):\n",
    "    retriever = store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": k, \"fetch_k\": fetch_k, \"lambda_mult\": lambda_mult}\n",
    "    )\n",
    "    reranked_docs = retriever.get_relevant_documents(query)\n",
    "    print(\"\\nMMR Reranked Docs:\")\n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        print(f\"Doc {i+1} Preview: {doc.page_content[:100]}...\")\n",
    "    return retriever, reranked_docs\n",
    "\n",
    "\n",
    "# 5. Answer with LLM\n",
    "def generate_llm_answer(retriever, query):\n",
    "    qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), retriever=retriever)\n",
    "    answer = qa.run(query)\n",
    "    print(\"\\nLLM Answer:\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# 6. Save answer to DOCX\n",
    "def save_to_docx(answer, filename):\n",
    "    docx = Document()\n",
    "    docx.add_heading(\"Answer from LLM\", 0)\n",
    "    docx.add_paragraph(answer)\n",
    "    docx.save(filename)\n",
    "\n",
    "\n",
    "# === Main runner function ===\n",
    "def run_pipeline(file_path, query):\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=768)\n",
    "    split_docs = load_and_split_pdf(file_path)\n",
    "\n",
    "    index_types = [\"FLAT\", \"IVF_FLAT\", \"HNSW\"]\n",
    "    for index_type in index_types:\n",
    "        print(f\"\\n===== Testing Index Type: {index_type} =====\")\n",
    "\n",
    "        store = create_vector_store(index_type, split_docs, embeddings)\n",
    "        perform_similarity_search(store, query)\n",
    "        retriever, _ = rerank_with_mmr(store, query)\n",
    "        answer = generate_llm_answer(retriever, query)\n",
    "        save_to_docx(answer, f\"answer_{index_type}.docx\")\n",
    "\n",
    "\n",
    "# === Run with actual inputs ===\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\n",
    "        file_path=\"/Users/ankita/Documents/Krish Naik Academy/Agentic Batch 2/RAG Assignment/ISLR.pdf\",\n",
    "        query=\"What is regularization in machine learning?\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bf5c4",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "* When I used a chunk size below 1000, I encountered an error. I need to investigate and resolve this issue.\n",
    "* TokenTextSplitter was not used for semantic search in this implementation.\n",
    "* Among three Milvus index types; HNSW had the fastest retrieval time , followed by IVF_FLAT and FLAT.\n",
    "* The average similarity scores were comparable across all index types, indicating consistent retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387f93f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
